{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name - Andrea Fox\n",
    "Date - July 12, 2020\n",
    "Class - DSC550-T301\n",
    "Assignment - Week 6 Exercises - Classification\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The goal of this part is to write a script to compute the decision tree for an input dataset. You can assume that all attributes are numeric, except for the last attribute which is the class.\n",
    "\n",
    "You should use the Information Gain based on Entropy for computing the best split value for each attribute. For the stopping criteria at a node, stop if the purity is at least 95% or stop if the node size is five or lower.\n",
    "\n",
    "Note that the best way to implement the splits for numeric attributes is to sort the values of that attribute from smallest to largest. Then you can use the mid-point between two distinct (consecutive) values as the split test of the form A≤v. You can then update the class frequencies on both sides of the split and compute the split entropy for each decision. After comparing with the entropy for the node, you can find the best split for the current attribute. Now repeat the whole process for each numeric attribute at that node, and choose the best split over all attributes. Finally, split the data according to the best split, and repeat the whole method recursively, until the stopping conditions are met.\n",
    "\n",
    "The decision tree should be printed in the following format:\n",
    "\n",
    "Decision: Car <= 1.5 , Gain= 0.4591479\n",
    "\n",
    "Decision: Age <= 22.5 , Gain= 0.9182958\n",
    "\n",
    "Leaf: label= H purity= 1 size= 1\n",
    "\n",
    "Leaf: label= L purity= 1 size= 2\n",
    "\n",
    "Leaf: label= H purity= 1 size= 3\n",
    "\n",
    "Note that each internal node, print the decision followed by the Information Gain, and for each leaf, print the majority label, purity of the leaf, and the size. The indentation indicates the tree level. All nodes at the same level of indentation (tabs) are at the same level in the tree. For the tree above, Car<=1.5 is the root decision. Its left child is Age<=22.5, and its right child is a leaf. Also, for Age≤22.5 its left and right children appear immediately below it.\n",
    "\n",
    "You may test your program on the iris.txt dataset.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tree structure has 5 nodes and has the following tree structure: \n",
      "\n",
      "Decision: petal width (cm) <= 0.800000011920929 Gain = 1.2516291673878228\n",
      "\tLeaf: Label = setosa Purity = 1.0 Size = 50\n",
      "\tDecision: petal width (cm) <= 1.75 Gain = 0.6780726131133057\n",
      "\t\tLeaf: Label = versicolor Purity = 0.5549351429491614 Size = 54\n",
      "\t\tLeaf: Label = virginica Purity = 0.8489030294828863 Size = 46\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference: https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py\n",
    "Sam helped with the math getting entropy\n",
    "\"\"\"\n",
    "\n",
    "#Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "#Function to find information gain based on entropy\n",
    "def compute_info_gain(entropies, values, nodes):\n",
    "    p1 = values[nodes[1]].sum()/values[nodes[0]].sum()\n",
    "    p2 = values[nodes[2]].sum()/values[nodes[0]].sum()\n",
    "    entropy_split = p1*entropies[nodes[1]] + p1*entropies[nodes[2]]\n",
    "    information_gain = entropies[nodes[0]] - entropy_split\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "#Function to return entropy for each node along with values by class for each node\n",
    "def get_entropies(tree_values):\n",
    "    node_splits = tree.tree_.value\n",
    "    entropies = []\n",
    "    values = []\n",
    "    for node in node_splits:\n",
    "        entropy = 0\n",
    "        values.append(node[0])\n",
    "        for classification in node[0]:\n",
    "            proportion = classification/node.sum()\n",
    "            if proportion != 0:\n",
    "                entropy += (-1)*(proportion*math.log(proportion, 2))\n",
    "        entropies.append(entropy)\n",
    "    return entropies, values\n",
    "\n",
    "#Load in dataset to use\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data[:, :], columns = iris.feature_names[:])\n",
    "y = pd.DataFrame(iris.target, columns = ['Species'])\n",
    "\n",
    "#Fitting the DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 5, min_impurity_decrease = 0.09)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Using those arrays, we can parse the tree structure:\n",
    "n_nodes = tree.tree_.node_count\n",
    "children_left = tree.tree_.children_left\n",
    "children_right = tree.tree_.children_right\n",
    "feature = tree.tree_.feature\n",
    "threshold = tree.tree_.threshold\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "        \n",
    "entropies, values = get_entropies(tree.tree_.value)\n",
    "\n",
    "numclass = tree.tree_.n_classes[0]\n",
    "class_names = iris.target_names\n",
    "node_labels = []\n",
    "for node in tree.tree_.value:\n",
    "    label_i = tree.classes_[np.argmax(node)]\n",
    "    label_name = class_names[label_i]\n",
    "    node_labels.append(label_name)\n",
    "    \n",
    "#Main function to print out the tree    \n",
    "def main():\n",
    "    try:\n",
    "        print(\"The tree structure has %s nodes and has the following tree structure: \\n\" % n_nodes)\n",
    "        for i in range(n_nodes):\n",
    "            if is_leaves[i]:\n",
    "                print(\"%sLeaf: Label = %s Purity = %s Size = %s\"\n",
    "                     % (node_depth[i] * \"\\t\",\n",
    "                       node_labels[i],\n",
    "                       (1-tree.tree_.impurity[i]), \n",
    "                       tree.tree_.n_node_samples[i]))\n",
    "            else:\n",
    "                nodes = [i, i+1, i+2]\n",
    "                information_gain = compute_info_gain(entropies, values, nodes)\n",
    "                print(\"%sDecision: %s <= %s Gain = %s\"\n",
    "                     % (node_depth[i] * \"\\t\",\n",
    "                       X.columns[feature[i]],\n",
    "                       threshold[i],\n",
    "                       information_gain\n",
    "                       ))\n",
    "    #Exception built in case of error            \n",
    "    except Exception as exception:\n",
    "        print('exception')\n",
    "        # print the traceback of the exception\n",
    "        traceback.print_exc()\n",
    "        # list name of exception and any arguments\n",
    "        print('An exception of type {0} occurred.'.format(type(exception).__name__, exception.args));\n",
    "        \n",
    "main()\n",
    "#Stopped here as I met one of the requirements of 5 nodes or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Your goal is to learn a SVM in the traditional dual formulation for the iris-slwc.txt dataset. This is a simple 2D dataset, consisting of 2 dimensions (the sepal length and width), and the third column is the class (+1,-1). One of the class corresponds to iris-setosa, and the other class to other types of irises.\n",
    "\n",
    "Implement the stochastic gradient ascent algorithm 21.1 in chapter 21, with three different kernels, namely, the linear kernel, the inhomogeneous quadratic kernel, and the homogeneous quadratic kernel. Use ϵ=0.0001 and C=10, and hinge loss.\n",
    "\n",
    "At the end, print all values of non-zero αi, i.e., for the support vectors, in the following format:\n",
    "\n",
    "i,αi, one per line.\n",
    "\n",
    "You should also print the number of support vectors.\n",
    "\n",
    "Do this for both the kernels. The results on the linear kernel should approximately match the hyperplane h10 in example 21.7.\n",
    "\n",
    "To check when the quadratic kernel is useful. You may try the quadratic kernel on the iris-PC.txt data. The results should match those given in Example 21.8.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference:https://github.com/dianejdan/SVM-Gradient-Ascent/blob/master/svm-gradient-ascent.py\n",
    "Torrey walked through this on Tuesday call. Let us know that original was missing one of the kernels, remove loc, diag\n",
    "need np. in front of it to work correctly.\n",
    "\"\"\"\n",
    "#Import libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "#Read in iris-slwc\n",
    "df = pd.read_csv('iris-slwc.txt', header = None)\n",
    "myData = np.genfromtxt('iris-slwc.txt', delimiter = ',')\n",
    "\n",
    "#sample size\n",
    "n = myData.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vectors\n",
      "\n",
      "Sample 25 : 4.9, 2.5; Class : 1.0; A : 9.178461657479335\n",
      "\n",
      "Sample 55 : 5.4, 3.4; Class : -1.0; A : 2.2640815245195807\n",
      "\n",
      "Sample 66 : 5.4, 3.0; Class : 1.0; A : 10.0\n",
      "\n",
      "Sample 107 : 4.9, 3.0; Class : -1.0; A : 10.0\n",
      "\n",
      "Sample 109 : 4.5, 2.3; Class : -1.0; A : 10.0\n",
      "\n",
      "Sample 122 : 5.0, 3.0; Class : -1.0; A : 10.0\n",
      "\n",
      "Sample 138 : 6.0, 3.4; Class : 1.0; A : 10.0\n",
      "\n",
      "Sample 146 : 5.4, 3.4; Class : -1.0; A : 2.724996110297604e-15\n",
      "\n",
      "Total Number of Support Vectors : 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change last column to all 1's\n",
    "X = myData.copy()\n",
    "X[:,-1] = 1\n",
    "C = 10 #Moved up from reference code to use in K\n",
    "\n",
    "#Function to build kerlnel for hinge loss\n",
    "def kernel_matrix(x, typ, C):\n",
    "    K_ = np.zeros(shape=(x.shape[0], x.shape[0]))\n",
    "    \n",
    "    #Linear Kernel\n",
    "    if typ == 'linear':\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[0]):\n",
    "                K_[i,j] = np.dot(x[i,], x[j,])\n",
    "                \n",
    "    #Homogeneous Kernel\n",
    "    elif typ == 'homogeneous':\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[0]):\n",
    "                K_[i,j] = np.square(np.dot(x[i,], x[j,]))\n",
    "    \n",
    "    #Inhomogeneous Kernel\n",
    "    elif typ == 'inhomogeneous': \n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[0]):\n",
    "                K_[i,j] = np.square(C + np.dot(x[i,], x[j,]))\n",
    "    return K_\n",
    "\n",
    "K = kernel_matrix(X, 'linear', C)\n",
    "\n",
    "#Calculate step size\n",
    "eta = 1/np.diag(K)\n",
    "\n",
    "t = 0\n",
    "\n",
    "#Initial alphas\n",
    "alpha = np.zeros(n)\n",
    "\n",
    "#Difference\n",
    "diff = 1\n",
    "eps = 0.0001\n",
    "\n",
    "while (diff > eps):\n",
    "    alpha0 = alpha.copy()\n",
    "    for k in range(n):\n",
    "        alpha[k] = alpha[k] + eta[k]*(1-myData[k,2]*sum(alpha*myData[:,2]*K[:,k]))\n",
    "        if alpha[k] < 0:\n",
    "            alpha[k] = 0\n",
    "        if alpha[k] > C:\n",
    "            alpha[k] = C\n",
    "    t = t+1\n",
    "    diff = np.sum([la.norm(alpha-alpha0)])\n",
    "    \n",
    "#Printing out information\n",
    "print('Support vectors\\n')\n",
    "for k in range(n):\n",
    "    if alpha[k] != 0:\n",
    "        print('Sample {} : {}, {}; Class : {}; A : {}\\n'.format(k+1, myData[k,0], myData[k,1], myData[k,2], alpha[k]))\n",
    "\n",
    "print('Total Number of Support Vectors : {}\\n'.format(sum(alpha!=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the second part of this question it sounded like it wasn't a must, so hopefully I didn't make a mistake reading it like that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Complete the Following Exercises in Your Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining of Massive Datasets - Page 462 --- Exercise 12.2.1: (c), (d)\n",
    "```\n",
    "Modify the training set of Fig.12.6 so that example b also includes the word \"nigeria\" (yet remains a negative example - perhaps someone telling about their trip to Nigeria). Find a weight vector that separates the positive and negative exmaples using:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>viagra</th>\n",
       "      <th>the</th>\n",
       "      <th>of</th>\n",
       "      <th>nigeria</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  viagra  the  of  nigeria  y\n",
       "a    1       1    0   1        1  1\n",
       "b    0       0    1   1        0 -1\n",
       "c    0       1    1   0        0  1\n",
       "d    1       0    0   1        0 -1\n",
       "e    1       0    1   0        1  1\n",
       "f    1       0    1   1        0 -1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Creating dict to hold values from Figure 12.6\n",
    "train = {'and': [1,0,0,1,1,1], \n",
    "         'viagra': [1,0,1,0,0,0], \n",
    "         'the': [0,1,1,0,1,1], \n",
    "         'of': [1,1,0,1,0,1], \n",
    "         'nigeria': [1,0,0,0,1,0],\n",
    "        'y': [1,-1,1,-1,1,-1]}\n",
    "\n",
    "#Creating dataframe\n",
    "spam = pd.DataFrame(train, index = ['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (c) - The basic method with a variable threshold, as suggested in Section 12.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row a Weights: [0.5 0.5 0.  0.5 0.5]\n",
      "Row b Weights: [ 0.5  0.5 -0.5  0.   0.5]\n",
      "Row c Weights: [0.5 1.  0.  0.  0.5]\n",
      "Row d Weights: [ 0.   1.   0.  -0.5  0.5]\n",
      "Row e Weights: [ 0.   1.   0.  -0.5  0.5]\n",
      "Row f Weights: [ 0.   1.   0.  -0.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference: Pg 451  from book and got some help from Sam\n",
    "\"\"\"\n",
    "#weights\n",
    "w = np.array([0 for x in range(5)])\n",
    "\n",
    "#learn rate\n",
    "n = 0.5\n",
    "\n",
    "#Calculate\n",
    "for index, row in spam.iterrows():\n",
    "    #training vector\n",
    "    x = np.array(row[:-1])\n",
    "    #training classification\n",
    "    y = row[-1]\n",
    "    wx = np.dot(x, w)\n",
    "    sign = ((wx > 0) == (y > 0))\n",
    "    if (sign == False) or (wx ==0):\n",
    "        #Update weights\n",
    "        w = w + n*y*x\n",
    "    print('Row {} Weights: {}'.format(index, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (d) - The Winnow method with a variable thershold, as suggested in Section 12.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,) and (6,) not aligned: 5 (dim 0) != 6 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e6351304adf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#training classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mwx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwx\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlower_fact\u001b[0m \u001b[1;31m#update threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,) and (6,) not aligned: 5 (dim 0) != 6 (dim 0)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference: Pg 455 from the book as well as some explanations/help from Sam and Rachel\n",
    "\"\"\"\n",
    "#weight\n",
    "w = np.array([1 for x in range(6)], dtype = 'f')\n",
    "converged = False\n",
    "\n",
    "#Tracking number of steps\n",
    "steps = 0\n",
    "\n",
    "#How much to raise threshold\n",
    "raise_fact = 2\n",
    "#How much to lower threshold\n",
    "lower_fast = 0.5\n",
    "\n",
    "while converged == False:\n",
    "    for index, row in spam.iterrows():\n",
    "        #training vector\n",
    "        x = np.array(row[:-1])\n",
    "        #training classification\n",
    "        y = row[-1]\n",
    "        wx = np.dot(x, w)\n",
    "        if (wx <= 0) and (y > 0):\n",
    "            w[-1] = w[-1]*lower_fact #update threshold\n",
    "            for i in range(len(x)-1):\n",
    "                if x[i] == 1:\n",
    "                    w[i] = raise_fact*w[i] #update weight\n",
    "        elif (wx >= 0) and (y < 0):\n",
    "            w[-1] = w[-1]*raise_fact #update threshold\n",
    "            for i in range(len(x)-1):\n",
    "                if x[i]==1:\n",
    "                    w[i] = w[i]*lower_fact #update weight\n",
    "                    \n",
    "        steps +=1\n",
    "        #Checking whether it converged or not\n",
    "        converged = check_convergence(spam, w)\n",
    "        if converged == True:\n",
    "            break\n",
    "        if steps == 1000:\n",
    "            break\n",
    "    if steps == 1000:\n",
    "        print('Did not converge')\n",
    "        break\n",
    "        \n",
    "#Print out the steps and weights\n",
    "print('Steps: {} Weights: {}'.format(steps, w))\n",
    "\n",
    "#Cannot seem to get the correct output. I've tried looking into some other resources, but wasn't able to figure it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining of Massive Datasets - Page 473 --- Exercise 12.3.2: (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The following training set obeys the rule that the positive examples all have vectors whose components sum to 10 or more, while the sum is less than 10 for the negative examples:\n",
    "([3,4,5], +1)\n",
    "([1,2,3], -1)\n",
    "([2,7,2], +1)\n",
    "([3,3,2], -1)\n",
    "([5,5,5], +1)\n",
    "([2,4,1], -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (a) - Which of these six vectors are the support vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference:\n",
    "https://chrisalbon.com/machine_learning/support_vector_machines/find_support_vectors/\n",
    "Sam sent reference\n",
    "\"\"\"\n",
    "\n",
    "# Load libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26726124, -0.74199852, -0.65465367],\n",
       "       [-0.53452248, -0.10599979, -1.30930734],\n",
       "       [ 0.26726124, -0.10599979,  1.30930734],\n",
       "       [-0.53452248,  1.8019964 , -0.65465367]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "X = np.array([[3,4,5],[2,7,2],[5,5,5],[1,2,3],[3,3,2],[2,4,1]])\n",
    "y = np.array([1,1,1,-1,-1,-1])\n",
    "\n",
    "# Standarize features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# Create support vector classifier object\n",
    "svc = SVC(kernel='linear', random_state=0)\n",
    "\n",
    "# Train classifier\n",
    "model = svc.fit(X_std, y)\n",
    "\n",
    "# View support vectors\n",
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View indices of support vectors\n",
    "model.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are Support Vectors:\n",
      "[3 3 2]\n",
      "[2 4 1]\n",
      "[3 4 5]\n",
      "[2 7 2]\n"
     ]
    }
   ],
   "source": [
    "# View the support vectors for each class\n",
    "print('The following are Support Vectors:')\n",
    "for index in model.support_:\n",
    "    print('{}'.format(X[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining of Massive Datasets - Page 481 --- Exercise 12.4.3: (a), (b)\n",
    "```\n",
    "Consider the one-dimensional training set\n",
    "(1,1), (2,2), (4,3), (8,4), (16,5), (32,6)\n",
    "Describe the function f(q), the label that is returned in response to the query q, when the interpolation used is:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "\n",
    "#Create an array to hold training set\n",
    "points = np.array([[1,1], [2,2], [4,3], [8,4], [16,5], [32,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (a) - The label of the nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "References: Sam helped me a ton, attempted https://mmas.github.io/interpolation-scipy but couldn't get it to work \n",
    "with the numbers I wanted to use.\n",
    "\"\"\"\n",
    "#Function using train (x,y) and returns y for q based on 1 nearest neighbor\n",
    "def interpolation_nn(train, q):\n",
    "    #I played around with the number some and it didn't change so I just decided 50\n",
    "    nearest_dist = 50\n",
    "    for point in train:\n",
    "        x = point[0]\n",
    "        y = point[1]\n",
    "        if abs(q - x) < nearest_dist:\n",
    "            nearest_dist = abs(q-x)\n",
    "            label = y\n",
    "    return label\n",
    "\n",
    "#This was mentioned in our Tuesday call I believe and there were some posts on it. \n",
    "#I saw some examples with linspace, but couldn't figure it out \n",
    "x = np.arange(0, 33, 0.01)\n",
    "y = np.array([])\n",
    "#Loop to update y array\n",
    "for q in x:\n",
    "    y = np.append(y, interpolation_nn(points, q))\n",
    "\n",
    "#Plotting the data. I also saw in examples plotting the points, real values, and interpolation but again couldn't make it work    \n",
    "plt.plot(x, y)\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('label')\n",
    "plt.title('1D Nearest Neighbor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (b) - The average of the labels of the two nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAer0lEQVR4nO3debhcVZnv8e+PJEAkYcwRYsIhKrQtemXwNGAHISAqKoLa6MVWIYI3YmuL02OL3RcBpR/ldit6aY2hsQVlFMGONA70IxHQC3qCYQhBjBhMTDBhyiAIJHnvH3tVUhR1pkqtqtpVv8/z1HP2VHu/tc85+62119prKSIwM7PetV27AzAzs/ZyIjAz63FOBGZmPc6JwMysxzkRmJn1OCcCM7Me50Rg1uMkLZY0a5TbLpN0zBDrZkla0dTgrCWcCHqUpAWSHpO0Q7tjaQZJ/yLpN5LWS7pP0snDbDtLUkj6t5rlt0qanT3YMUi/p/cNs35G+iz/VbP825LOHs0xIuJlEbFg2yK1MnMi6EGSZgCvBgI4PtMxxufY7zD+BLwZ2AU4BfiypL8eYfuT07nIqkXn4jBJM1twnJZow99PT3Mi6E0nA7cB36S4aAIg6TBJD0kaV7XsrZLuStPbSfqUpN9KekTS1ZJ2T+sq30xPk/R74Cdp+XfSPtdKulnSy6r2vYek70taJ+mXkj4n6daq9X8p6UZJj0r6taR3DPWBIuIzEXFfRGyOiNuBW4BXDXMOHk+f/zNDbSDpVElLUsnpR5L2qVr3ZUnLU+wLJb26at3Zkq5J38rXAbNHOHc7pm0fkfR4Ohd7SjqPImFfKGmDpAuH+TznA58b5rMcJ2lR2v/PJb2iat2W2z2SJkq6JH3mJZI+Wed2z4GS7kq/06sk7VhzrE9Lejjt911Vy3eRdKmkNZIelPRPkrZL62ZL+pmkL0l6FDhb0r6SfpqO87Ckq4b5/LYtIsKvHnsBS4G/A14JPAPsWbXut8Brq+a/A3wqTX+EIoFMB3YAvg5ckdbNoChhXArsBExMy08FJqftLwAWVe37yvR6HrA/sBy4Na3bKc2/FxgPHAw8DLxsFJ9vIrAKOHaI9bOAFcBewDrgJWn5rcDsNP2WdJ5emo7/T8DPq/bxbmCPtO7jwEPAjmnd2em8voXiy9bEEc7d+4Hvp/MwLv1edk7rFgDvG+azVs77JOAPwDFp+beBs9P0wcBq4NC0/1OAZcAOaf2yqvd9HvgpsFuK9S5gRdXxlgG/AF4A7A4sAU6vOq8bgS+mz3gkRcmrcn4vBf4z/T3MAO4HTkvrZqf3/n06pxOBK4B/TOdwR+Dwdv/vdOur7QH41eJfOByeLlJT0vx9wEer1n8O+Eaanpz+kfdJ80uA11RtOzXta3zVBelFwxx717TNLumC9EzlIlF17Eoi+J/ALTXv/zrwmVF8xkuAHwIaYv2sysWN4pv0VWm6OhH8oHKRSvPbAU9UzkWdfT4GHJCmzwZurlk/3Lk7Ffg58Io6+13A6BLBeIrkfltaXp0IvgZ8tuZ9vwaOTNPL2JoIHgBeX7Xd+3huInh31fz5wNyq87oR2Klq/dXA/06/76eA/avWvR9YkKZnA7+vifFSYB4wvd3/N93+8q2h3nMK8OOIeDjNX07V7aE0/7ZUifw24I6IeDCt2we4Lt1eeJzi4rYJ2LPq/csrE5LGSfp8uh2yjuIiAjAF6KO4eC2v9950rEMrx0rHexfFt/ghSfo/wMuBd0S6mozgC8DrJR1Qs3wfinqGyrEfBQRMS8f5eLp1sjat3yV9rnqfpbK/oc7dt4AfAVdKWinpfEkTRhF7rYuAPSW9uc6xP15zLvem+FZf6wUM/TupeKhq+gmK0kjFYxHxp6r5B9M+pwDbp/nqddOGOdYnKc75L1S0bDq1TizWBK6Q6SGSJgLvAMZJqvwz7wDsKumAiLgzIu6V9CDwBuBvKRJDxXLg1Ij4WZ19z0iT1RffvwVOAI6hSAK7UHxzFrCG4tvjdIpbBFBcnKqP9dOIeO0YPt85Ke4jI2LdaN4TEY9IugD4bM2q5cB5EXFZneO8GvgH4DXA4ojYLKnyubbsus7+6p675BzgnHQeb6D4xn5xnf0M91meSefgs8DiOp/lvFHsZhXF7+TeNL/3MNvWs5uknaqSQT9wD8VtvWcoktK9Vev+UP0RqncUEQ8B/wtA0uHAf0u6OSKWjjEmG4FLBL3lLRTfQvcHDkyvl1JUrFY3t7wc+DBwBEUdQcVc4LxKpamkPkknDHO8yRS3Ax6huP/9z5UVEbEJuJaiUvB5kv6yJobrgb+Q9B5JE9LrryS9tN6BJJ1JkXheGxGPjHAean0R+GuKc1H9Wc9UqtxOFZ1vr/pcGymS2XhJZwE7j3CMIc+dpKMk/Q8VlfTrKC6Ym9L7/gi8aAyf5VsUyf3YqmUXAadLOlSFnSS9SdLkOu+/On3u3SRNAz40hmNXnCNp+5QwjwO+k37fV1Ocg8npPHyM4hZWXZLeLml6mn2MIlFsGmp7a5wTQW85BfiPiPh9RDxUeQEXAu/S1iZ7V1Dc7/1J1S0kgC8D84EfS1pPUfl56DDHu5Si+P8Him+Bt9Ws/xBFKeEhigvYFRSJg4hYD7wOOAlYmbb5AsVFrp5/pviG+ZvUwmaDpE8PdzIqUunhfIrKz8qy69Lxrky3te6hKG1AcRvnBxQlmQeBP1P/Fkq14c7dXsA1FElgCUVl7ber3ndiasXzlVF8lk0ULaGqP8sgxTfrCykuqEsp7snXcy5FRfrvgP9OcT010nGrPJSOsRK4jKIi+b607u8p6pweoKiPuRz4xjD7+ivgdkkbKM7dGRHxuzHEYqOk0d1GNctP0heAvSLilBE3tpaQ9AHgpIg4st2xWD4uEVjbqHhO4BXpdsUhwGnAde2Oq5dJmipppornHl5C0TTWv5Mu58pia6fJFLeDXkDRzv1fKdqZW/tsT9FM94UUD91dCXy1rRFZdr41ZGbW43xryMysx5Xu1tCUKVNixowZ7Q7DzKxUFi5c+HBE9NVbV7pEMGPGDAYHB9sdhplZqaQHRevyrSEzsx7nRGBm1uOcCMzMepwTgZlZj3MiMDPrcVlbDUlaBqyn6DFwY0QM1KyfRfEkaaUjqWsj4tycMZmZ2bO1ovnoUTU9WNa6JSKOa0EcZmZWR+meIzCz+n790Hr+666V7Q7DMhqYsTtH/EXdZ8K2Se5EEBT9rwfw9YiYV2ebV0m6k6L/8k9ExOLaDSTNAeYA9Pf354zXrLQuuuUBrlm4Amnkba2cTj/yxaVMBDMjYqWk5wM3SrovIm6uWn8HxWDgGyS9EfgesF/tTlICmQcwMDDgXvLM6ti0Oejf/Xnc/Mmj2h2KlUzWVkMRsTL9XE3Rp/khNevXRcSGNH0DMEHSlOfsyMxG5J6ErVHZEkEaF3VyZZpi2MF7arbZSyoKsmlgku0oxrc1szEK8G0ha0jOW0N7Atel6/x44PKI+KGk0wEiYi5wIvABSRuBJymGxPPXGrMGRIDzgDUiWyKIiAeAA+osn1s1fSHFgNpmto2KEoFTgY2dnyw26xIR4RKBNcSJwKxLBPjekDXEicCsW7iOwBrkRGDWJYJwHYE1xInArEu41ZA1yonArEtE+DkCa4wTgVmXCAK5TGANcCIw6xIuEVijnAjMzHqcE4FZl/CTxdYoJwKzLuFWQ9YoJwKzrhGuI7CGOBGYdQlXFlujnAjMukSAm49aQ5wIzLqEh/KwRjkRmHUJj1BmjcqaCCQtk3S3pEWSBuusl6SvSFoq6S5JB+eMx6ybudWQNSrnUJUVR0XEw0OsewOwX3odCnwt/TSzMSrGI3AqsLFr962hE4BLo3AbsKukqW2OyayUPEKZNSp3Igjgx5IWSppTZ/00YHnV/Iq07FkkzZE0KGlwzZo1mUI1Kz8XCKwRuRPBzIg4mOIW0AclHVGzvt6f7XOaPkTEvIgYiIiBvr6+HHGalZ7rCKxRWRNBRKxMP1cD1wGH1GyyAti7an46sDJnTGbdyiOUWaOyJQJJO0maXJkGXgfcU7PZfODk1HroMGBtRKzKFZNZN3OJwBqVs9XQnsB16RvKeODyiPihpNMBImIucAPwRmAp8ATw3ozxmHU1dzFhjcqWCCLiAeCAOsvnVk0H8MFcMZj1Eo9QZo1qd/NRM2uSKDobMhszJwKzLuI8YI1wIjDrEu5ryBrlRGDWLcLdUFtjnAjMukR4hDJrkBOBWZdw81FrlBOBWZfwCGXWKCcCsy7hEcqsUU4EZl3CrYasUU4EZl3CBQJrlBOBWZcoSgQuEtjYORGYdQuPUGYNciIw6xKuI7BGORGYdQmPR2CNciIw6xIeocwa5URg1iVcIrBGZU8EksZJ+pWk6+usmy1pjaRF6fW+3PGYdSt3MWGNyjlUZcUZwBJg5yHWXxURH2pBHNZiT2/czNonn2l3GD1j4+bNuExgjciaCCRNB94EnAd8LOexrPO84+v/j0XLH293GD1lv+dPbncIVkK5SwQXAJ8Ehvvr/BtJRwD3Ax+NiOW1G0iaA8wB6O/vzxGnZbBq7ZO8cp/deMtB09odSs84fN8p7Q7BSihbIpB0HLA6IhZKmjXEZt8HroiIpySdDlwCHF27UUTMA+YBDAwM+EH6koiA/Z4/ifcctk+7QzGzYeSsLJ4JHC9pGXAlcLSkb1dvEBGPRMRTafYi4JUZ47EW8wNOZuWQLRFExJkRMT0iZgAnAT+JiHdXbyNpatXs8RSVytYlik7QnAnMOl0rWg09i6RzgcGImA98WNLxwEbgUWB2q+OxnDx0olkZtCQRRMQCYEGaPqtq+ZnAma2IwVrPDziZlYOfLLZsXEdgVg5OBJZNRHgMXbMScCKwbFwiMCsHJwLLynnArPM5EVg2RSdoTgVmnc6JwLIJj6ZuVgpOBJaN6wjMysGJwPIJ3GrIrAScCCwblwjMysGJwLIpniMws07nRGDZuERgVg5OBJaNm4+alYMTgWUTuPmoWRk4EVg27n3UrBycCCwbj0tjVg7ZE4GkcZJ+Jen6Out2kHSVpKWSbpc0I3c81kJ+jsCsFFpRIjiDoYegPA14LCL2Bb4EfKEF8ViLhEcoMyuFrIlA0nTgTcC/D7HJCcAlafoa4DVyM5Ou4ToCs3LIXSK4APgksHmI9dOA5QARsRFYC+xRu5GkOZIGJQ2uWbMmV6zWZH6OwKwcsiUCSccBqyNi4XCb1Vn2nDaHETEvIgYiYqCvr69pMVpeHqHMrBxylghmAsdLWgZcCRwt6ds126wA9gaQNB7YBXg0Y0zWQi4RmJVDtkQQEWdGxPSImAGcBPwkIt5ds9l84JQ0fWLaxk8hdQnXEZiVw/hWH1DSucBgRMwHLga+JWkpRUngpFbHY5m5SGDW8VqSCCJiAbAgTZ9VtfzPwNtbEYO1VqVg5zRg1vn8ZLFlUbnB5wKBWedzIrAsKhU9bjVk1vmcCCyLLbeGnAfMOp4TgWWxtURgZp3OicCycB2BWXk4EVgWlUFp3HWUWedzIrAs/FigWXk4EVhWLhCYdT4nAsvKzUfNOp8TgWXhymKz8hi2iwlJbxtufURc29xwrFtsqSxucxxmNrKR+hp68zDrAnAisLpcIjArj2ETQUS8t1WBWHdxFxNm5TGqOgJJe0q6WNIP0vz+kk7LG5qVmbuYMCuP0VYWfxP4EfCCNH8/8JEcAVl38GMEZuUx2kQwJSKuJg1Cnwaa35QtKiu9rXUELhKYdbrRJoI/SdqD9EVP0mHA2uHeIGlHSb+QdKekxZLOqbPNbElrJC1Kr/eN+RNYZ6okgvZGYWajMNoRyj5GMb7wiyX9DOijGGN4OE8BR0fEBkkTgFsl/SAibqvZ7qqI+NCYoraOF745ZFYao0oEEXGHpCOBl1B8yft1RDwzwnsC2JBmJ6SXrw4NOP+H9/G7h//U7jDG5JlNmwFXFpuVwagSgaQdgb8DDqe4mN8iaW4ac3i4940DFgL7Av8WEbfX2exvJB1BUQH90YhYXmc/c4A5AP39/aMJuWs8vXEzX13wW/bYaXv2mLR9u8MZk/2n7sxB/bu1OwwzG8Fobw1dCqwH/m+afyfwLUYYeD4iNgEHStoVuE7SyyPinqpNvg9cERFPSToduAQ4us5+5gHzAAYGBnqqVFG5xXLq4S/kg0ft2+ZozKwbjTYRvCQiDqiav0nSnaM9SEQ8LmkBcCxwT9XyR6o2uwj4wmj32SvcnbOZ5TbaVkO/Si2FAJB0KPCz4d4gqS+VBJA0ETgGuK9mm6lVs8cDS0YZT8/xvXYzy2WkTufupqgTmACcLOn3aX4f4N4R9j0VuCTVE2wHXB0R10s6FxiMiPnAhyUdD2wEHgVmb8uH6UZb2uO7IaaZZTLSraHjGt1xRNwFHFRn+VlV02cCZzZ6jF6wdcjHNgdiZl1rpE7nHqyel/R8YMesEdmzuI7AzHIbbadzx0v6DfA74KfAMuAHGeOyZGsvnmZmeYy2svizwGHA/RHxQuA1jFBZbM3hXjzNLLfRJoJnUlPP7SRtFxE3AQdmjMsS9+tvZrmN9jmCxyVNAm4GLpO0mqKlj2Xmkb7MLLfRlghOAJ4EPgr8EPgtww9jac3iymIzy2y0nc5V93h2SaZYrI6tzUddJDCzPEZ6oGw99b+TiqKD0Z2zRGVbhPv1N7PMRnqOYHKrArH6tlQWOxOYWSajrSOwNtnSfLTNcZhZ93IiKAnXEZhZLk4EHc63hswsNyeCDufKYjPLzYmgw20ZBN5FAjPLxImg07lEYGaZORF0ONcRmFlu2RKBpB0l/ULSnZIWSzqnzjY7SLpK0lJJt0uakSuesvIIZWaWW84SwVPA0WnQ+wOBY6vHPU5OAx6LiH2BL+HB65/DI5SZWW7ZEkEUNqTZCelV213FCWztu+ga4DVyg/lncashM8stax2BpHGSFgGrgRsj4vaaTaYBywEiYiOwFtijzn7mSBqUNLhmzZqcIXcc1xGYWW5ZE0FEbIqIA4HpwCGSXl6zSb3L23M6uYuIeRExEBEDfX19OULtWFu7mHAmMLM8WtJqKCIeBxYAx9asWgHsDSBpPLAL8GgrYiqL8KDFZpZZzlZDfZJ2TdMTgWOA+2o2mw+ckqZPBH4SER6KpQ7nATPLZbRDVTZiKnCJpHEUCefqiLhe0rnAYETMBy4GviVpKUVJ4KSM8ZTS1qEqnQrMLI9siSAi7gIOqrP8rKrpPwNvzxVDN9jSfLTNcZhZ9/KTxR3Og9ebWW5OBB3OzUfNLDcngg7n5qNmlpsTQYdzicDMcnMiMDPrcU4EHc5PVZhZbk4EHa/S+6jvDZlZHk4EHc69j5pZbk4EHc6VxWaWmxNBh/MIZWaWmxNBh/MIZWaWmxNBh3MdgZnl5kTQ4dzXkJnl5kTQ4QKPTGNmeTkRdDiXCMwst5wjlO0t6SZJSyQtlnRGnW1mSVoraVF6nVVvX+bygJnlk3OEso3AxyPiDkmTgYWSboyIe2u2uyUijssYR6l5hDIzyy3nCGWrgFVper2kJcA0oDYRdIWlq9ezZv3TTd/vAw9vAFwiMLN8cpYItpA0g2LYytvrrH6VpDuBlcAnImJxnffPAeYA9Pf35wu0QRue2sjrL7iFTZvz9RA3aceW/KrMrAdlv7pImgR8F/hIRKyrWX0HsE9EbJD0RuB7wH61+4iIecA8gIGBgY7rj/PJpzexaXPw3pkzeN3+ezV9/xO3H8cB03dp+n7NzCBzIpA0gSIJXBYR19aur04MEXGDpK9KmhIRD+eMq9kqTTxf1DeJV714jzZHY2Y2NjlbDQm4GFgSEV8cYpu90nZIOiTF80iumLLx079mVmI5SwQzgfcAd0talJZ9GugHiIi5wInAByRtBJ4ETooo31As7iHUzMosZ6uhWxnhS3JEXAhcmCuGVnEPoWZWZn6yuAncQ6iZlZkTQRO4h1AzKzMngiZwHYGZlZkTQRNU6rddR2BmZeRE0AThnqLNrMScCJrIecDMysiJoAncQ6iZlZkTQRNsaT7a5jjMzBrhRNAEHkXMzMrMicDMrMc5ETSBnyMwszJzImgCP0dgZmXmRNAELhGYWZk5ETRB+TrONjPbyomgKSq9j7pIYGbl40TQBO591MzKLOdQlXtLuknSEkmLJZ1RZxtJ+oqkpZLuknRwrnhych2BmZVZzqEqNwIfj4g7JE0GFkq6MSLurdrmDcB+6XUo8LX0s1Q8QpmZlVm2EkFErIqIO9L0emAJMK1msxOAS6NwG7CrpKm5YsrFI5SZWZm1pI5A0gzgIOD2mlXTgOVV8yt4brJA0hxJg5IG16xZkyvMhrmOwMzKLHsikDQJ+C7wkYhYV7u6zlue0xgzIuZFxEBEDPT19eUIc5u4ryEzK7OsiUDSBIokcFlEXFtnkxXA3lXz04GVOWPKIfDINGZWXjlbDQm4GFgSEV8cYrP5wMmp9dBhwNqIWJUrplxcIjCzMsvZamgm8B7gbkmL0rJPA/0AETEXuAF4I7AUeAJ4b8Z4snMeMLMyypYIIuJWRrg2RtFb2wdzxdAqHqHMzMrMTxY3gUcoM7MycyJoAtcRmFmZORE0gbuYMLMycyJognA/1GZWYk4ETbD1KQIXCcysfJwImiD8PJmZlZgTQVO41ZCZlZcTQRP4OQIzKzMngibwnSEzKzMngibwcwRmVmZOBE1QaT7qVkNmVkZOBE3gB8rMrMycCJrAI5SZWZk5ETSTM4GZlZATQRNs7X3UmcDMyseJoBncasjMSiznUJXfkLRa0j1DrJ8laa2kRel1Vq5YcvNzBGZWZjmHqvwmcCFw6TDb3BIRx2WMoSX8ZLGZlVnOoSpvljQj1/7H6qf3r+Fz19+bZd9PPL0J8K0hMyunnCWC0XiVpDuBlcAnImJxvY0kzQHmAPT39zd0oEk7jGe/PSc1GueIZu67By+dunO2/ZuZ5aKcg6qkEsH1EfHyOut2BjZHxAZJbwS+HBH7jbTPgYGBGBwcbHqsZmbdTNLCiBiot65trYYiYl1EbEjTNwATJE1pVzxmZr2qbYlA0l5KtauSDkmxPNKueMzMelW2OgJJVwCzgCmSVgCfASYARMRc4ETgA5I2Ak8CJ4UH/zUza7mcrYbeOcL6Cymal5qZWRv5yWIzsx7nRGBm1uOcCMzMepwTgZlZj8v6QFkOktYADzb49inAw00Mp5XKGrvjbi3H3VplinufiOirt6J0iWBbSBoc6sm6TlfW2B13aznu1ipr3LV8a8jMrMc5EZiZ9bheSwTz2h3ANihr7I67tRx3a5U17mfpqToCMzN7rl4rEZiZWQ0nAjOzHtcziUDSsZJ+LWmppE+1O57RkrRM0t2SFknq2BF5JH1D0mpJ91Qt213SjZJ+k37u1s4YhzJE7GdL+kM674vS4EkdQ9Lekm6StETSYklnpOUdfc6HibujzzeApB0l/ULSnSn2c9LyF0q6PZ3zqyRt3+5Yx6on6ggkjQPuB14LrAB+CbwzIvIMYtxEkpYBAxHR0Q+tSDoC2ABcWhmRTtL5wKMR8fmUfHeLiH9oZ5z1DBH72cCGiPiXdsY2FElTgakRcYekycBC4C3AbDr4nA8T9zvo4PMNkMZP2SmNqjgBuBU4A/gYcG1EXClpLnBnRHytnbGOVa+UCA4BlkbEAxHxNHAlcEKbY+oqEXEz8GjN4hOAS9L0JRT/8B1niNg7WkSsiog70vR6YAkwjQ4/58PE3fGisCHNTkivAI4GrknLO+6cj0avJIJpwPKq+RWU5I+P4g/tx5IWSprT7mDGaM+IWAXFBQB4fpvjGasPSbor3TrqqFss1dLY4AcBt1Oic14TN5TgfEsaJ2kRsBq4Efgt8HhEbEyblOnaskWvJALVWVaWe2IzI+Jg4A3AB9NtDMvva8CLgQOBVcC/tjec+iRNAr4LfCQi1rU7ntGqE3cpzndEbIqIA4HpFHcaXlpvs9ZGte16JRGsAPaump8OrGxTLGMSESvTz9XAdRR/fGXxx3RPuHJveHWb4xm1iPhj+qffDFxEB573dJ/6u8BlEXFtWtzx57xe3GU439Ui4nFgAXAYsKukymiPpbm2VOuVRPBLYL9Uu789cBIwv80xjUjSTqlCDUk7Aa8D7hn+XR1lPnBKmj4F+M82xjImlYtp8lY67LynisuLgSUR8cWqVR19zoeKu9PPN4CkPkm7pumJwDEUdRw3UYzBDh14zkejJ1oNAaTmaBcA44BvRMR5bQ5pRJJeRFEKgGJ86cs7NW5JVwCzKLrl/SPwGeB7wNVAP/B74O0R0XGVskPEPoviNkUAy4D3V+69dwJJhwO3AHcDm9PiT1Pcb+/Ycz5M3O+kg883gKRXUFQGj6P4En11RJyb/k+vBHYHfgW8OyKeal+kY9czicDMzOrrlVtDZmY2BCcCM7Me50RgZtbjnAjMzHqcE4GZWY9zIjAz63FOBGZmPW78yJuY2Ugk/SNwMkXnhmuAhZ3cpbJZNScCs20k6ZUU3ZYcRPE/dQdFP/tmpeBEYLbtXg1cFxFPAEjq+H6szKq5jsCsOdxXi5WWE4HZtrsZeKukiam32De3OyCzsfCtIbNtlMbfvQpYBDxI0bumWWm491GzJuv0ge/NavnWkJlZj3OJwMysx7lEYGbW45wIzMx6nBOBmVmPcyIwM+txTgRmZj3u/wNBFrjRTAWplgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference: Again got help from Sam. He broke it down enough for it to make sense \n",
    "\"\"\"\n",
    "#Function similar to (a)\n",
    "def interpolation_nn_2(train, q):\n",
    "    distances = []\n",
    "    for point in train:\n",
    "        x = point[0]\n",
    "        y = point[1]\n",
    "        distances.append(abs(q-x))\n",
    "    \n",
    "    #Again played with distance numbers and it did not affect the outcome that I could see\n",
    "    smallest = 100\n",
    "    smallest_i = 0\n",
    "    sec_smallest = 50\n",
    "    sec_smallest_i = 0\n",
    "    for i in range(len(distances)):\n",
    "        if distances[i] < smallest:\n",
    "            smallest = distances[i]\n",
    "            smallest_i = i\n",
    "    for i in range(len(distances)):\n",
    "        if distances[i] < sec_smallest and i != smallest_i:\n",
    "            sec_smallest = distances[i]\n",
    "            sec_smallest_i = i\n",
    "    \n",
    "    #Getting the average of the 2 nearest neighbors        \n",
    "    avg_of_labels = (train[smallest_i][1] + train[sec_smallest_i][1]) / 2\n",
    "    return avg_of_labels\n",
    "\n",
    "#Did the same as above using arnage instead of linspace\n",
    "x = np.arange(0, 33, 0.01)\n",
    "y = np.array([])\n",
    "#Loop to update y array\n",
    "for q in x:\n",
    "    y = np.append(y, interpolation_nn_2(points, q))\n",
    "\n",
    "#Plotting the interpolation line\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('label')\n",
    "plt.title('Average 2 Nearest Neighbors')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
