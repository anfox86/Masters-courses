{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instant-superior",
   "metadata": {},
   "source": [
    "```\n",
    "Name: Andrea Fox\n",
    "Date: April 29, 2021\n",
    "Course: DSC650 - T301 Big Data\n",
    "Assignment 07\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "urban-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import hashlib\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pygeohash\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fatal-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "#set directories\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "results_dir = current_dir.joinpath('results')\n",
    "\n",
    "if results_dir.exists():\n",
    "    shutil.rmtree(results_dir)\n",
    "results_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "#Pulled from assignment03\n",
    "def read_jsonl_data():\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-balance",
   "metadata": {},
   "source": [
    "## 7.1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-insulin",
   "metadata": {},
   "source": [
    "```\n",
    "Start by loading the dataset from the previous assignment using Pandas's read_parquet method. Next, add the concatenated key then using Panda's apply method to create a new column called key. For this part of the example, we will create 16 partitions so that we can compare it to the partitions we create from hashed keys in the next part of the assignment. The partitions are determined by the first letter of the composite key using the following partitions.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "domestic-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Worked with Jolene on this who took some samples from winter term slack channel\n",
    "#create function to flatten record\n",
    "def flatten_record(record):\n",
    "    flat_record = dict()\n",
    "    for key, value in record.items():\n",
    "        if key in ['airline', 'src_airport', 'dst_airport']:\n",
    "            if isinstance(value, dict):\n",
    "                for child_key, child_value in value.items():\n",
    "                    flat_key = '{}_{}'.format(key, child_key)\n",
    "                    flat_record[flat_key] = child_value\n",
    "        else:\n",
    "            flat_record[key] = value\n",
    "            \n",
    "    return flat_record\n",
    "\n",
    "#create function for flatten dataset\n",
    "def create_flattened_dataset():\n",
    "    records = read_jsonl_data()\n",
    "    parquet_path = results_dir.joinpath('routes-flattened.parquet')\n",
    "    return pd.DataFrame.from_records([flatten_record(record) for record in records])\n",
    "\n",
    "#create dataframe and column key\n",
    "df = create_flattened_dataset()\n",
    "df['key'] = df['src_airport_iata'].astype(str) + df['dst_airport_iata'].astype(str) + df['airline_iata'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "technical-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create partitions\n",
    "partitions = (\n",
    "        ('A', 'A'), ('B', 'B'), ('C', 'D'), ('E', 'F'),\n",
    "        ('G', 'H'), ('I', 'J'), ('K', 'L'), ('M', 'M'),\n",
    "        ('N', 'N'), ('O', 'P'), ('Q', 'R'), ('S', 'T'),\n",
    "        ('U', 'U'), ('V', 'V'), ('W', 'X'), ('Y', 'Z')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "together-clinic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'A'): 'A',\n",
       " ('B', 'B'): 'B',\n",
       " ('C', 'D'): 'C-D',\n",
       " ('E', 'F'): 'E-F',\n",
       " ('G', 'H'): 'G-H',\n",
       " ('I', 'J'): 'I-J',\n",
       " ('K', 'L'): 'K-L',\n",
       " ('M', 'M'): 'M',\n",
       " ('N', 'N'): 'N',\n",
       " ('O', 'P'): 'O-P',\n",
       " ('Q', 'R'): 'Q-R',\n",
       " ('S', 'T'): 'S-T',\n",
       " ('U', 'U'): 'U',\n",
       " ('V', 'V'): 'V',\n",
       " ('W', 'X'): 'W-X',\n",
       " ('Y', 'Z'): 'Y-Z'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new key kv_key\n",
    "partition_dict = {}\n",
    "for key in partitions:\n",
    "    if key[0] == key[1]:\n",
    "        kv_key = key[0]\n",
    "    else:\n",
    "        kv_key = key[0] + '-' + key[1]\n",
    "    partition_dict[key] = kv_key\n",
    "    \n",
    "#wanted to make sure it looked correct\n",
    "partition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "economic-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create functon to get_key\n",
    "def get_key(s_key):\n",
    "    for key, value in partition_dict.items():\n",
    "        if s_key[0] == key[0] or s_key[0] == key[1]:\n",
    "            return value\n",
    "    return ' '\n",
    "\n",
    "#add kv_key column\n",
    "df['kv_key'] = df['key'].apply(get_key)\n",
    "\n",
    "#tested to make sure it worked\n",
    "df.to_csv('test', sep = ',') #downloaded it and opened in excel and looked accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fewer-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to_parquet method with partition_cols = ['kv_key'] to save partitioned dataset\n",
    "df.to_parquet(os.getcwd() + '/results/kv.parquet', partition_cols = ['kv_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-stylus",
   "metadata": {},
   "source": [
    "## 7.1 b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-america",
   "metadata": {},
   "source": [
    "```\n",
    "Next, we are going to partition the dataset again, but this time we will partition by the hash value of the key. The following is a function that will create a SHA256 hash of the input key and return a hexadecimal string representation of the hash.\n",
    "\n",
    "We will partition the data using the first character of the hexadecimal hash. As such, there are 16 possible partitions. Create a new column called hashed that is a hashed value of the key column. Next, create a partitioned dataset based on the first character of the hashed key and save the results to results/hash.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "increased-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries for part b\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "million-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SHA256 hash of the input key and return hexadecimal string rep of hash\n",
    "def hash_key(key):\n",
    "    m = hashlib.sha256()\n",
    "    m.update(str(key).encode('utf-8'))\n",
    "    return m.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "enormous-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create hashed and hash_key column. Found an old example for hashed and hash_key\n",
    "df['key'] = df['src_airport_iata'].astype(str) + df['dst_airport_iata'].astype(str) + df['airline_iata'].astype(str)\n",
    "df['hashed'] = df.apply(lambda x: hash_key(x.key), axis=1)\n",
    "df['hash_key'] = df['hashed'].str[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-proceeding",
   "metadata": {},
   "source": [
    "```\n",
    "I did this several times and had to remove the hash.parquet a couple of times because I had forgotten to create hashed column. Then tried mimicking what I did previously but still wasn't getting what I wanted. Used an old example to finally arrive at this solution, and it was mentioned on general channel by Robert Zacchigna.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "neutral-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "#created csv to test it worked\n",
    "df.to_csv('hash_test1', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dramatic-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the to_parquet again but changing partition_cols = hash_key instead of kv_key like previous section\n",
    "df.to_parquet(os.getcwd() + '/results/hash.parquet', partition_cols = ['hash_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-domestic",
   "metadata": {},
   "source": [
    "## 7.1 c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-excerpt",
   "metadata": {},
   "source": [
    "```\n",
    "Finally, we will simulate multiple geographically distributed data centers. For this example, we will assume we have three data centers located in the western, central, and eastern United States. Google lists the locations of their data centers and we will use the following locations for our three data centers.\n",
    "\n",
    "West\n",
    "The Dalles, Oregon\n",
    "Latitude: 45.5945645\n",
    "Longitude: -121.1786823\n",
    "Central\n",
    "Papillion, NE\n",
    "Latitude: 41.1544433\n",
    "Longitude: -96.0422378\n",
    "East\n",
    "Loudoun County, Virginia\n",
    "Latitude: 39.08344\n",
    "Longitude: -77.6497145\n",
    "Assume that you have an application that provides routes for each of the source airports and you want to store routes in the data center closest to the source airport. The output folders should look as follows.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "filled-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geolib in /opt/conda/lib/python3.8/site-packages (1.0.7)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from geolib) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install geolib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "transparent-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries needed for c\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.neighbors\n",
    "from geolib import geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "pressed-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['src_airport_geohash'] = df.apply(\n",
    "    lambda row: pygeohash.encode(row.src_airport_latitude, row.src_airport_longitude), axis=1\n",
    ")\n",
    "def determine_location(src_airport_geohash):\n",
    "    locations = dict(\n",
    "        central = pygeohash.encode(41.1544433, -96.0422378),\n",
    "        east = pygeohash.encode(39.08344, -77.6497145),\n",
    "        west = pygeohash.encode(45.5945645, -121.1786823)\n",
    "    )\n",
    "    #Got this from Corinne\n",
    "    distances = []\n",
    "    for location, geohash in locations.items():\n",
    "        hav = pygeohash.geohash_haversine_distance(src_airport_geohash, geohash)\n",
    "        distances.append(tuple((hav, location)))\n",
    "    \n",
    "    \n",
    "    distances.sort()\n",
    "    return distances[0][1]\n",
    "df['location'] = df['src_airport_geohash'].apply(determine_location)\n",
    "\n",
    "#Create csv to verify it looks accurate\n",
    "df.to_csv('geo_test', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "acute-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('results/geo', partition_cols=['location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-demand",
   "metadata": {},
   "source": [
    "## 7.1 d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-lithuania",
   "metadata": {},
   "source": [
    "```\n",
    "Create a Python function that takes as input a list of keys and the number of partitions and returns a list of keys sorted into the specified number of partitions. The partitions should be roughly equal in size. Furthermore, the partitions should have the property that each partition contains all the keys between the least key in the partition and the greatest key in the partition. In other words, the partitions should be ordered.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "living-hello",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cat': 1}, {'chicken': 1}, {'cow': 1}, {'dog': 1}, {'donkey': 2}, {'duck': 2}, {'goose': 2}, {'horse': 2}, {'mouse': 3}, {'pig': 3}, {'rabbit': 3}]\n"
     ]
    }
   ],
   "source": [
    "#Used some code from github as reference as well as this website https://www.geeksforgeeks.org/partition-problem-dp-18/\n",
    "def balance_partitions (keys, num_partitions):\n",
    "    vals = sorted(set(keys))\n",
    "    num_vals = len(vals)\n",
    "    partition_counts = (num_vals / num_partitions)+1\n",
    "    partitions  = []\n",
    "    x = 1\n",
    "    y = 1\n",
    "    for i in range(num_vals):\n",
    "        key_val ={}\n",
    "        if x <= partition_counts:\n",
    "            key_val[vals[i]] = y\n",
    "            x = x + 1\n",
    "        else:\n",
    "            x = 1\n",
    "            y = y + 1\n",
    "            key_val[vals[i]] = y\n",
    "            x = x + 1\n",
    "        partitions.append(key_val)\n",
    "    return partitions\n",
    "\n",
    "#create list of keys\n",
    "keys = ['duck', 'chicken', 'pig', 'rabbit', 'horse', 'cow', 'donkey', 'cat', 'dog', 'goose', 'mouse']\n",
    "#set number of partitions\n",
    "num_partitions = 3\n",
    "\n",
    "#create partitions and then print\n",
    "partitions  = balance_partitions(keys, num_partitions)\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "handmade-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cat': 1}, {'chicken': 1}, {'cow': 1}, {'dog': 1}, {'donkey': 1}, {'duck': 1}, {'goose': 2}, {'horse': 2}, {'mouse': 2}, {'pig': 2}, {'rabbit': 2}]\n"
     ]
    }
   ],
   "source": [
    "#change number of paritions\n",
    "num_partitions = 2\n",
    "\n",
    "#create partitions and then print\n",
    "partitions = balance_partitions(keys, num_partitions)\n",
    "print(partitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
