{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "from fastavro import writer, reader, parse_schema\n",
    "from fastavro.schema import load_schema\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl_data():\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'airline': {'airline_id': 410,\n",
       "   'name': 'Aerocondor',\n",
       "   'alias': 'ANA All Nippon Airways',\n",
       "   'iata': '2B',\n",
       "   'icao': 'ARD',\n",
       "   'callsign': 'AEROCONDOR',\n",
       "   'country': 'Portugal',\n",
       "   'active': True},\n",
       "  'src_airport': {'airport_id': 2965,\n",
       "   'name': 'Sochi International Airport',\n",
       "   'city': 'Sochi',\n",
       "   'country': 'Russia',\n",
       "   'iata': 'AER',\n",
       "   'icao': 'URSS',\n",
       "   'latitude': 43.449902,\n",
       "   'longitude': 39.9566,\n",
       "   'altitude': 89,\n",
       "   'timezone': 3.0,\n",
       "   'dst': 'N',\n",
       "   'tz_id': 'Europe/Moscow',\n",
       "   'type': 'airport',\n",
       "   'source': 'OurAirports'},\n",
       "  'dst_airport': {'airport_id': 2990,\n",
       "   'name': 'Kazan International Airport',\n",
       "   'city': 'Kazan',\n",
       "   'country': 'Russia',\n",
       "   'iata': 'KZN',\n",
       "   'icao': 'UWKD',\n",
       "   'latitude': 55.606201171875,\n",
       "   'longitude': 49.278701782227,\n",
       "   'altitude': 411,\n",
       "   'timezone': 3.0,\n",
       "   'dst': 'N',\n",
       "   'tz_id': 'Europe/Moscow',\n",
       "   'type': 'airport',\n",
       "   'source': 'OurAirports'},\n",
       "  'codeshare': False,\n",
       "  'equipment': ['CR2']}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = read_jsonl_data()\n",
    "records[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Created schema based off of record printed above.\n",
    "# Used https://json-schema.org/understanding-json-schema/reference/numeric.html to figure out number\n",
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    validation_csv_path = results_dir.joinpath('validation-results.csv')\n",
    "    with open(validation_csv_path, 'w') as f:\n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                jsonschema.validate(instance = records[1], schema = schema)\n",
    "                pass\n",
    "            except ValidationError as e:\n",
    "                print('exception')\n",
    "                traceback.print_exc()\n",
    "                print('An exception of type {0} occurred.'.format(type(exception).__name__, exception.args));\n",
    "            # Used code that I was taught to check for errors. Not sure I'm doing this correctly as my validation file is \n",
    "            # empty. Sam was telling me I needed to do something with writer.writerow but I couldn't make it work correctly.\n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    parsed_schema = load_schema(schema_path)\n",
    "    #Had a hard time with parse_schema so used load_schema instead https://fastavro.readthedocs.io/en/latest/schema.html\n",
    "    with open(data_path, 'wb') as out:\n",
    "        writer(out, parsed_schema, records)\n",
    "    \n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             airline  \\\n",
      "0  {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
      "\n",
      "                                         src_airport  \\\n",
      "0  {'airport_id': 2965, 'name': 'Sochi Internatio...   \n",
      "\n",
      "                                         dst_airport  codeshare  stops  \\\n",
      "0  {'airport_id': 2990, 'name': 'Kazan Internatio...      False      0   \n",
      "\n",
      "  equipment  \n",
      "0     [CR2]  \n"
     ]
    }
   ],
   "source": [
    "# Mimicking what Adam Curry did from Teams to check if it loaded correctly\n",
    "import fastavro\n",
    "data_path = results_dir.joinpath('routes.avro')\n",
    "with open(data_path, mode = 'rb') as fp:\n",
    "    reader = fastavro.reader(fp)\n",
    "    records = [r for r in reader]\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7f96019154a0>\n",
       "  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n",
       "  num_columns: 38\n",
       "  num_rows: 67663\n",
       "  num_row_groups: 1\n",
       "  format_version: 1.0\n",
       "  serialized_size: 7571"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            pass\n",
    "            ## TODO: Use Apache Arrow to create Parquet table and save the dataset\n",
    "            table = read_json(f)\n",
    "    pq.write_table(table, parquet_output_path)\n",
    "    \n",
    "create_parquet_dataset()\n",
    "\n",
    "# Jolene helped me with this part. I had the above, but no good way to check that it worked without opening the file itself.\n",
    "# Which wasn't possible on here. I downloaded it, and opened in notepad but it looked like a bunch of characters.\n",
    "parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "\n",
    "pq_f = pq.ParquetFile(parquet_output_path)\n",
    "pq_f.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ParquetSchema object at 0x7f96019daa00>\n",
       "required group field_id=0 schema {\n",
       "  optional group field_id=1 airline {\n",
       "    optional int64 field_id=2 airline_id;\n",
       "    optional binary field_id=3 name (String);\n",
       "    optional binary field_id=4 alias (String);\n",
       "    optional binary field_id=5 iata (String);\n",
       "    optional binary field_id=6 icao (String);\n",
       "    optional binary field_id=7 callsign (String);\n",
       "    optional binary field_id=8 country (String);\n",
       "    optional boolean field_id=9 active;\n",
       "  }\n",
       "  optional group field_id=10 src_airport {\n",
       "    optional int64 field_id=11 airport_id;\n",
       "    optional binary field_id=12 name (String);\n",
       "    optional binary field_id=13 city (String);\n",
       "    optional binary field_id=14 country (String);\n",
       "    optional binary field_id=15 iata (String);\n",
       "    optional binary field_id=16 icao (String);\n",
       "    optional double field_id=17 latitude;\n",
       "    optional double field_id=18 longitude;\n",
       "    optional int64 field_id=19 altitude;\n",
       "    optional double field_id=20 timezone;\n",
       "    optional binary field_id=21 dst (String);\n",
       "    optional binary field_id=22 tz_id (String);\n",
       "    optional binary field_id=23 type (String);\n",
       "    optional binary field_id=24 source (String);\n",
       "  }\n",
       "  optional group field_id=25 dst_airport {\n",
       "    optional int64 field_id=26 airport_id;\n",
       "    optional binary field_id=27 name (String);\n",
       "    optional binary field_id=28 city (String);\n",
       "    optional binary field_id=29 country (String);\n",
       "    optional binary field_id=30 iata (String);\n",
       "    optional binary field_id=31 icao (String);\n",
       "    optional double field_id=32 latitude;\n",
       "    optional double field_id=33 longitude;\n",
       "    optional int64 field_id=34 altitude;\n",
       "    optional double field_id=35 timezone;\n",
       "    optional binary field_id=36 dst (String);\n",
       "    optional binary field_id=37 tz_id (String);\n",
       "    optional binary field_id=38 type (String);\n",
       "    optional binary field_id=39 source (String);\n",
       "  }\n",
       "  optional boolean field_id=40 codeshare;\n",
       "  optional group field_id=41 equipment (List) {\n",
       "    repeated group field_id=42 list {\n",
       "      optional binary field_id=43 item (String);\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also provided by Jolene. This was nice that I could see everything here. I definitely need to work on being able to add\n",
    "# this to my own code on \n",
    "pq_f.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    if not airline.get('name'):\n",
    "        return None\n",
    "    if not airline.get('airline_id'):\n",
    "        return None\n",
    "    if not airline.get('active'):\n",
    "        return None\n",
    "    \n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    obj.name = airline.get('name')\n",
    "    # Got the code from professor hints, but changed name to mimic the alias and then expanded for the other parts. Going back \n",
    "    # to the schema and record I printed above it breaks it down like this under airline.\n",
    "    if airline.get('name'):\n",
    "        obj.name = airline.get('name')\n",
    "    if airline.get('alias'):\n",
    "        obj.alias = airline.get('alias')\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('callsign'):\n",
    "        obj.callsign = airline.get('callsign')\n",
    "    if airline.get('country'):\n",
    "        obj.country = airline.get('country')\n",
    "    if airline.get('active'):\n",
    "        obj.active = airline.get('active')\n",
    "        \n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        airline = _airline_to_proto_obj(record.get('airline', {}))\n",
    "        if airline:\n",
    "            route.airline.CopyFrom(airline)\n",
    "        src_airport = _airport_to_proto_obj(record.get('src_airport', {}))\n",
    "        if src_airport:\n",
    "            route.src_airport.CopyFrom(src_airport)\n",
    "        dst_airport = _airport_to_proto_obj(record.get('dst_airport', {}))\n",
    "        if dst_airport:\n",
    "            route.dst_airport.CopyFrom(dst_airport)\n",
    "        route.codeshare = record.get('codeshare') # Sam provided this tidbit in the chat\n",
    "        routes.route.append(route) \n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))   \n",
    "\n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JSON Schema</th>\n",
       "      <th>Avro</th>\n",
       "      <th>Parquet</th>\n",
       "      <th>Protocol Buffer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3385</td>\n",
       "      <td>3191</td>\n",
       "      <td>1975469</td>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   JSON Schema  Avro  Parquet  Protocol Buffer\n",
       "0         3385  3191  1975469             1073"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the output sizes of the different formats\n",
    "\n",
    "# Defining path to files\n",
    "j_schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "a_schema_path = schema_dir.joinpath('routes.avsc')\n",
    "par_path = results_dir.joinpath('routes.parquet')\n",
    "p_schema_path = schema_dir.joinpath('routes.proto')\n",
    "\n",
    "# Getting size for the paths defined above\n",
    "j_schema_size = os.path.getsize(j_schema_path)\n",
    "a_schema_size = os.path.getsize(a_schema_path)\n",
    "par_size = os.path.getsize(par_path)\n",
    "p_schema_size = os.path.getsize(p_schema_path)\n",
    "\n",
    "# Create a dataframe out of the sizes\n",
    "df = pd.DataFrame({'JSON Schema': [j_schema_size], 'Avro': [a_schema_size], 'Parquet': [par_size], 'Protocol Buffer': [p_schema_size]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append comparison file with dataframe\n",
    "compare = results_dir.joinpath('comparison.csv')\n",
    "with open (compare, 'w') as f:\n",
    "    df.to_csv(f, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    for record in records:\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        if src_airport:\n",
    "            latitude = src_airport.get('latitude')\n",
    "            longitude = src_airport.get('longitude')\n",
    "            if latitude and longitude:\n",
    "                h = pygeohash.encode(latitude,longitude)\n",
    "                record['geohash'] = h\n",
    "                hashes.append(h)\n",
    "                #todo: use pygeohash.encode() to assign geohashes to the rcords and complete the hashes list\n",
    "    # Provided by professor on 3/31\n",
    "    hashes.sort()\n",
    "    three_letter = sorted(list(set([entry[:3] for entry in hashes])))\n",
    "    hash_index = {value: [] for value in three_letter}\n",
    "    \n",
    "    for record in records:\n",
    "        geohash = record.get('geohash')\n",
    "        if geohash:\n",
    "            hash_index[geohash[:3]].append(record)\n",
    "    \n",
    "    for key, values in hash_index.items():\n",
    "        output_dir = geoindex_dir.joinpath(str(key[:1])).joinpath(str(key[:2]))\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        output_path = output_dir.joinpath('{}.jsonl.gz'.format(key))\n",
    "        with gzip.open(output_path, 'w') as f:\n",
    "            json_output = '\\n'.join([json.dumps(value) for value in values])\n",
    "            f.write(json_output.encode('utf-8'))\n",
    "    \n",
    "            \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eppley Airfield\n"
     ]
    }
   ],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    h = pygeohash.encode(latitude,longitude)\n",
    "    dist = 0\n",
    "    name = ''\n",
    "    for i, record in enumerate(records):\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        if src_airport:\n",
    "            latitude2 = src_airport.get('latitude')\n",
    "            longitude2 = src_airport.get('longitude')\n",
    "            air_name = src_airport.get('name')\n",
    "            if latitude2 and longitude2:\n",
    "                h2 = pygeohash.encode(latitude2, longitude2)\n",
    "                dist_n = pygeohash.geohash_approximate_distance(h, h2)\n",
    "                if i == 0:\n",
    "                    dist = dist_n\n",
    "                else:\n",
    "                    if dist>dist_n:\n",
    "                        dist = dist_n\n",
    "                        name = air_name\n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "    print(name)\n",
    "    \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
